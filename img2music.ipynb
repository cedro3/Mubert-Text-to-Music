{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/Mubert-Text-to-Music/blob/main/img2music.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30xPxDSDrJEl"
      },
      "outputs": [],
      "source": [
        "#@title **setup img2text (CLIP Interrogator)**\n",
        "\n",
        "# install library\n",
        "!pip3 install ftfy regex tqdm transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "!pip3 install git+https://github.com/openai/CLIP.git\n",
        "!git clone -b v1 https://github.com/pharmapsychotic/clip-interrogator.git\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "%cd /content/BLIP\n",
        "\n",
        "\n",
        "# import library\n",
        "import clip\n",
        "import gc\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "blip_image_eval_size = 384\n",
        "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'        \n",
        "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n",
        "blip_model.eval()\n",
        "blip_model = blip_model.to(device)\n",
        "\n",
        "\n",
        "# difine function\n",
        "def generate_caption(pil_image):\n",
        "    gpu_image = transforms.Compose([\n",
        "        transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "    return caption[0]\n",
        "\n",
        "def load_list(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def rank(model, image_features, text_array, top_count=1):\n",
        "    top_count = min(top_count, len(text_array))\n",
        "    text_tokens = clip.tokenize([text for text in text_array]).cuda()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = torch.zeros((1, len(text_array))).to(device)\n",
        "    for i in range(image_features.shape[0]):\n",
        "        similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "    similarity /= image_features.shape[0]\n",
        "\n",
        "    top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
        "    return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
        "\n",
        "def interrogate(image, models):\n",
        "    caption = generate_caption(image)\n",
        "    if len(models) == 0:\n",
        "        print(f\"\\n\\n{caption}\")\n",
        "        return\n",
        "\n",
        "    table = []\n",
        "    bests = [[('',0)]]*5\n",
        "    for model_name in models:\n",
        "        print(f\"Interrogating with {model_name}...\")\n",
        "        model, preprocess = clip.load(model_name)\n",
        "        model.cuda().eval()\n",
        "\n",
        "        images = preprocess(image).unsqueeze(0).cuda()\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        ranks = [\n",
        "            rank(model, image_features, mediums),\n",
        "            rank(model, image_features, [\"by \"+artist for artist in artists]),\n",
        "            rank(model, image_features, trending_list),\n",
        "            rank(model, image_features, movements),\n",
        "            rank(model, image_features, flavors, top_count=3)\n",
        "        ]\n",
        "\n",
        "        for i in range(len(ranks)):\n",
        "            confidence_sum = 0\n",
        "            for ci in range(len(ranks[i])):\n",
        "                confidence_sum += ranks[i][ci][1]\n",
        "            if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
        "                bests[i] = ranks[i]\n",
        "\n",
        "        row = [model_name]\n",
        "        for r in ranks:\n",
        "            row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
        "\n",
        "        table.append(row)\n",
        "\n",
        "        del model\n",
        "        gc.collect()\n",
        "    display(pd.DataFrame(table, columns=[\"Model\", \"Medium\", \"Artist\", \"Trending\", \"Movement\", \"Flavors\"]))\n",
        "\n",
        "    flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
        "    medium = bests[0][0][0]\n",
        "    if caption.startswith(medium):\n",
        "        text = f\"{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\" \n",
        "    else:\n",
        "        text = f\"{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\" \n",
        "    return text\n",
        "\n",
        "\n",
        "# setting\n",
        "data_path = \"../clip-interrogator/data/\"\n",
        "\n",
        "artists = load_list(os.path.join(data_path, 'artists.txt'))\n",
        "flavors = load_list(os.path.join(data_path, 'flavors.txt'))\n",
        "mediums = load_list(os.path.join(data_path, 'mediums.txt'))\n",
        "movements = load_list(os.path.join(data_path, 'movements.txt'))\n",
        "\n",
        "sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "trending_list = [site for site in sites]\n",
        "trending_list.extend([\"trending on \"+site for site in sites])\n",
        "trending_list.extend([\"featured on \"+site for site in sites])\n",
        "trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "\n",
        "# download sample pics\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1Mjwnr_m3pgxTPB7kusePjmKDHktGeV2w', 'pics.zip', quiet=False)\n",
        "! unzip pics.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **setup text2music (Mubert)**\n",
        "\n",
        "# install library\n",
        "! pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "! pip install -U sentence-transformers\n",
        "! pip install httpx\n",
        "\n",
        "\n",
        "# import library\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "mubert_tags_string = 'tribal,action,kids,neo-classic,run 130,pumped,jazz / funk,ethnic,dubtechno,reggae,acid jazz,liquidfunk,funk,witch house,tech house,underground,artists,mystical,disco,sensorium,r&b,agender,psychedelic trance / psytrance,peaceful,run 140,piano,run 160,setting,meditation,christmas,ambient,horror,cinematic,electro house,idm,bass,minimal,underscore,drums,glitchy,beautiful,technology,tribal house,country pop,jazz & funk,documentary,space,classical,valentines,chillstep,experimental,trap,new jack swing,drama,post-rock,tense,corporate,neutral,happy,analog,funky,spiritual,sberzvuk special,chill hop,dramatic,catchy,holidays,fitness 90,optimistic,orchestra,acid techno,energizing,romantic,minimal house,breaks,hyper pop,warm up,dreamy,dark,urban,microfunk,dub,nu disco,vogue,keys,hardcore,aggressive,indie,electro funk,beauty,relaxing,trance,pop,hiphop,soft,acoustic,chillrave / ethno-house,deep techno,angry,dance,fun,dubstep,tropical,latin pop,heroic,world music,inspirational,uplifting,atmosphere,art,epic,advertising,chillout,scary,spooky,slow ballad,saxophone,summer,erotic,jazzy,energy 100,kara mar,xmas,atmospheric,indie pop,hip-hop,yoga,reggaeton,lounge,travel,running,folk,chillrave & ethno-house,detective,darkambient,chill,fantasy,minimal techno,special,night,tropical house,downtempo,lullaby,meditative,upbeat,glitch hop,fitness,neurofunk,sexual,indie rock,future pop,jazz,cyberpunk,melancholic,happy hardcore,family / kids,synths,electric guitar,comedy,psychedelic trance & psytrance,edm,psychedelic rock,calm,zen,bells,podcast,melodic house,ethnic percussion,nature,heavy,bassline,indie dance,techno,drumnbass,synth pop,vaporwave,sad,8-bit,chillgressive,deep,orchestral,futuristic,hardtechno,nostalgic,big room,sci-fi,tutorial,joyful,pads,minimal 170,drill,ethnic 108,amusing,sleepy ambient,psychill,italo disco,lofi,house,acoustic guitar,bassline house,rock,k-pop,synthwave,deep house,electronica,gabber,nightlife,sport & fitness,road trip,celebration,electro,disco house,electronic'\n",
        "mubert_tags = np.array(mubert_tags_string.split(','))\n",
        "mubert_tags_embeddings = minilm.encode(mubert_tags)\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "import httpx\n",
        "import json\n",
        "\n",
        "\n",
        "# difine function\n",
        "def get_track_by_tags(tags, pat, duration, maxit=20, autoplay=False, loop=False):\n",
        "  if loop:\n",
        "    mode = \"loop\"\n",
        "  else:\n",
        "    mode = \"track\"\n",
        "  r = httpx.post('https://api-b2b.mubert.com/v2/RecordTrackTTM', \n",
        "      json={\n",
        "          \"method\":\"RecordTrackTTM\",\n",
        "          \"params\": {\n",
        "              \"pat\": pat, \n",
        "              \"duration\": duration,\n",
        "              \"tags\": tags,\n",
        "              \"mode\": mode\n",
        "          }\n",
        "      })\n",
        "\n",
        "  rdata = json.loads(r.text)\n",
        "  assert rdata['status'] == 1, rdata['error']['text']\n",
        "  trackurl = rdata['data']['tasks'][0]['download_link']\n",
        "\n",
        "  print('Generating track ', end='')\n",
        "  for i in range(maxit):\n",
        "      r = httpx.get(trackurl)\n",
        "      if r.status_code == 200:\n",
        "          display(Audio(trackurl, autoplay=autoplay))\n",
        "          break\n",
        "      time.sleep(1)\n",
        "      print('.', end='')\n",
        "\n",
        "def find_similar(em, embeddings, method='cosine'):\n",
        "    scores = []\n",
        "    for ref in embeddings:\n",
        "        if method == 'cosine': \n",
        "            scores.append(1 - np.dot(ref, em)/(np.linalg.norm(ref)*np.linalg.norm(em)))\n",
        "        if method == 'norm': \n",
        "            scores.append(np.linalg.norm(ref - em))\n",
        "    return np.array(scores), np.argsort(scores)\n",
        "\n",
        "def get_tags_for_prompts(prompts, top_n=3, debug=False):\n",
        "    prompts_embeddings = minilm.encode(prompts)\n",
        "    ret = []\n",
        "    for i, pe in enumerate(prompts_embeddings):\n",
        "        scores, idxs = find_similar(pe, mubert_tags_embeddings)\n",
        "        top_tags = mubert_tags[idxs[:top_n]]\n",
        "        top_prob = 1 - scores[idxs[:top_n]]\n",
        "        if debug:\n",
        "            print(f\"Prompt: {prompts[i]}\\nTags: {', '.join(top_tags)}\\nScores: {top_prob}\\n\\n\\n\")\n",
        "        ret.append((prompts[i], list(top_tags)))\n",
        "    return ret\n",
        "\n",
        "\n",
        "# get token\n",
        "email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "r = httpx.post('https://api-b2b.mubert.com/v2/GetServiceAccess', \n",
        "    json={\n",
        "        \"method\":\"GetServiceAccess\",\n",
        "        \"params\": {\n",
        "            \"email\": email,\n",
        "            \"license\":\"ttmmubertlicense#f0acYBenRcfeFpNT4wpYGaTQIyDI4mJGv5MfIhBFz97NXDwDNFHmMRsBSzmGsJwbTpP1A6i07AXcIeAHo5\",\n",
        "            \"token\":\"4951f6428e83172a4f39de05d5b3ab10d58560b8\",\n",
        "            \"mode\": \"loop\"\n",
        "        }\n",
        "    })\n",
        "\n",
        "rdata = json.loads(r.text)\n",
        "assert rdata['status'] == 1, \"probably incorrect e-mail\"\n",
        "pat = rdata['data']['pat']\n",
        "print(f'Got token: {pat}')"
      ],
      "metadata": {
        "id": "8qaZFR2KcbTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbDEMDGJrJEo"
      },
      "outputs": [],
      "source": [
        "#@title **img2text**\n",
        "\n",
        "img = \"02.jpg\" #@param {type:\"string\"}\n",
        "image_path ='pics/'+ img\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "thumb = image.copy()\n",
        "thumb.thumbnail([blip_image_eval_size, blip_image_eval_size])\n",
        "display(thumb)\n",
        "\n",
        "text = interrogate(image, models=['ViT-L/14'])\n",
        "print('text = ', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **text2musicðŸŽµ**\n",
        "import time\n",
        "\n",
        "print('text = ', text)\n",
        "prompt = text\n",
        "duration = 30 #@param {type:\"number\"}\n",
        "loop = False #@param {type:\"boolean\"}\n",
        "\n",
        "def generate_track_by_prompt(prompt, duration, loop=False):\n",
        "  _, tags = get_tags_for_prompts([prompt,])[0]\n",
        "  print('tags = ', tags)\n",
        "  \n",
        "  try:\n",
        "    get_track_by_tags(tags, pat, duration, autoplay=True, loop=loop)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "  print('\\n')\n",
        "\n",
        "generate_track_by_prompt(prompt, duration, loop)"
      ],
      "metadata": {
        "id": "0P4f2VNVWhoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d4dd9c310c32a31bb53615812f2f2c6cba010b7aa4dfb14e2b192e650667fecd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}